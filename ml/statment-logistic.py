import pandas
import math
from sklearn.metrics import roc_auc_score

# Загрузите данные из файла data-logistic.csv. Это двумерная выборка, целевая переменная на которой принимает
# значения -1 или 1.
data = pandas.read_csv(r'C:\Users\olllk\Downloads\data-logistic.csv', header=None)
y = data[0]
X = data.loc[:, 1:]


# Убедитесь, что выше выписаны правильные формулы для градиентного спуска. Обратите внимание,
# что мы используем полноценный градиентный спуск, а не его стохастический вариант!

def fw1(w1, w2, X, y, k, C):
    l = len(y)
    s = 0
    for i in range(0, l):
        s += y[i] * X[1][i] * (1.0 - 1.0 / (1 + math.exp(-y[i] * (w1 * X[1][i] + w2 * X[2][i]))))
    return w1 + (k * (1.0 / l) * s) - k * C * w1


def fw2(w1, w2, X, y, k, C):
    l = len(y)
    s = 0
    for i in range(0, l):
        s += y[i] * X[2][i] * (1.0 - 1.0 / (1 + math.exp(-y[i] * (w1 * X[1][i] + w2 * X[2][i]))))
    return w2 + (k * (1.0 / l) * s) - k * C * w2


# Реализуйте градиентный спуск для обычной и L2-регуляризованной (с коэффициентом регуляризации 10)
#  логистической регрессии. Используйте длину шага k=0.1. В качестве начального приближения используйте вектор (0, 0).

def gg(X, y, C=0.0, k=0.1, w1=0.0, w2=0.0, err=1e-5):
    imin = 0
    imax = 10000
    w1_new, w2_new = w1, w2
    while True:
        w1_new, w2_new = fw1(w1, w2, X, y, k, C), fw2(w1, w2, X, y, k, C)
        e = math.sqrt((w1_new - w1) ** 2 + (w2_new - w2) ** 2)
        imin += 1
        if imin >= imax or e <= err:
            break
        else:
            w1, w2 = w1_new, w2_new
    print(imin)
    return [w1_new, w2_new]


# Запустите градиентный спуск и доведите до сходимости (евклидово расстояние между векторами весов на соседних
# итерациях должно быть не больше 1e-5). Рекомендуется ограничить сверху число итераций десятью тысячами.

w1, w2 = gg(X, y)
rw1, rw2 = gg(X, y, 10.0)


# Какое значение принимает AUC-ROC на обучении без регуляризации и при ее использовании?
# Эти величины будут ответом на задание. В качестве ответа приведите два числа через пробел.
# Обратите внимание, что на вход функции roc_auc_score нужно подавать оценки вероятностей,
# подсчитанные обученным алгоритмом. Для этого воспользуйтесь сигмоидной функцией:
# a(x) = 1 / (1 + exp(-w1 x1 - w2 x2)).

def a(X, w1, w2):
    return 1.0 / (1.0 + math.exp(-w1 * X[1] - w2 * X[2]))


y_sc = X.apply(lambda cc: a(cc, w1, w2), axis=1)
y_rsc = X.apply(lambda rc: a(rc, rw1, rw2), axis=1)

auc_ = roc_auc_score(y, y_score=y_sc)
auc_reg = roc_auc_score(y, y_rsc)

print(auc_, auc_reg)
# на регуляризованной логрег длина шага не влияет на значение под кривой, при больше длине шага метод не сходится,
# при меньшей -  немного увеличивается
# 244 - regular, k=0.1      1479 - regular, k=0.01
# 8 - L2, k=0.1             47 - L2, k= 0.01
